#+TITLE: Central orthogonal composite design

(final source at [[file:src/interaction.py][src/orthogonal.py]], MIT license)

** Task description
Create central orthogonal composite design with linear and quadratic interactions, 
find linear regression (the model) and apply statistical estimates (Cochran's C test,
Student's t test, Fisher's F test) to find the model adequate to the original object.

** Initial conditions
For example, let's define the following factors bounds and their quantity:
#+BEGIN_SRC python :session orthogonal
import numpy as np

x_bounds = [np.array([1, 1]),
            np.array([-1, 6]),
            np.array([-10, 5]),
            np.array([-8, 2])]

def extend_inter(a):
    a.extend(
        [a[1] * a[2], a[1] * a[3], a[2] * a[3], a[1] * a[2] * a[3],
         a[1]**2, a[2]**2, a[3]**2]
    )
    return a

x_bounds = extend_inter(x_bounds)

factors = len(x_bounds)
#+END_SRC

For convenience, we add null-factor to the three main factors, which is always equal to one.
We define function =extend_inter()= to create bounds of interaction factors. 
To use it on =x_bounds=, we define every pair as =np.array=. In the composite design we 
add axial points, hence number of experiments equals to two to the power of three 
plus 6 additional combinations (+ and - axial for each factor) plus zero-combination (every factor is 0).
#+BEGIN_SRC python :session orthogonal
axial_n = 1.215
experiments = 15
#+END_SRC

Also, we define the repeats number of the same factors combination and confidence probability
for statistical estimates:
#+BEGIN_SRC python :session orthogonal
samples = 3
confidence_prob = 0.9
#+END_SRC

For experimentally aquired values we take randomly generated numbers in arbitrarily
defined bounds:
#+BEGIN_SRC python :session orthogonal
import random

y_bounds = [
    int(200 + np.mean([min(x_bounds[i]) for i in range(1, 4)])),
    int(200 + np.mean([max(x_bounds[i]) for i in range(1, 4)])),
]
y = [
        [random.randint(min(y_bounds), max(y_bounds)) for i in range(samples)]
        for j in range(experiments)
    ]
#+END_SRC

** Experiment design
Let's fill experiment matrix with factor values and response function values. From possible combinations
we choose only those, that have one at the beginning (null-factor). Also we define procedure to extend selected combinations
with linear and quadratic interactions.
#+BEGIN_SRC python :session orthogonal
import itertools

combinations = list(itertools.product([-1, 1], repeat=4))
xn = combinations[8:]
xn = list(map(list, xn))

for i in range(1, 4):
    axial_comb = [1, 0, 0, 0]
    xn.append([*axial_comb[:i], axial_n, *axial_comb[i+1:]])
    xn.append([*axial_comb[:i], -axial_n, *axial_comb[i+1:]])

xn.extend([[1, *( [0] * 3 )]])

xn = list(map(extend_inter, xn))
xn_col = np.array(list(zip(*xn)))
#+END_SRC

Decoded factor values are derived from encoded matrix. As with encoded values, we also
create column matrix. For further calculations we define change of factor (=delta_x=)
and zero level for each factor:
#+BEGIN_SRC python :session orthogonal
delta_x = [abs(x_bounds[i][0] - x_bounds[i][1]) / 2 for i in range(len(x_bounds))]
x0 = [(x_bounds[i][0] + x_bounds[i][1]) / 2 for i in range(len(x_bounds))]
x = [[] for i in range(len(xn))]
for i in range(len(xn)):
    for j in range(len(x_bounds)):
        if xn[i][j] == 0:
            x[i].append(x0[j])
        elif xn[i][j] == 1:
            x[i].append(max(x_bounds[j]))
        elif xn[i][j] == -1:
            x[i].append(min(x_bounds[j]))
        elif xn[i][j] > 1:
            x[i].append(axial_n * delta_x[j] + x0[j])
        else:
            x[i].append(-axial_n * delta_x[j] + x0[j])

x_col = np.array(list(zip(*x)))
#+END_SRC

For output formatting we use =prettytable= module. Let's define function to create experiment matrix:
#+BEGIN_SRC python :results output org :session orthogonal :exports both
from prettytable import PrettyTable

def create_matrix():
    table = PrettyTable()
    table_head = ["Experiment #"]
    for i in range(factors):
        table_head.append(f"x{i}")
    for i in range(samples):
        table_head.append(f"y{i+1}")
    table.field_names = table_head
    for i in range(experiments):
        table.add_row([i + 1, *np.round(x[i], 3), *y[i]])
    return table

matrix = create_matrix()
print(matrix)
#+END_SRC

#+RESULTS:
#+begin_src org
+--------------+-----+--------+---------+--------+------+------+------+-------+--------+---------+-------+-----+-----+-----+
| Experiment # |  x0 |   x1   |    x2   |   x3   |  x4  |  x5  |  x6  |   x7  |   x8   |    x9   |  x10  |  y1 |  y2 |  y3 |
+--------------+-----+--------+---------+--------+------+------+------+-------+--------+---------+-------+-----+-----+-----+
|      1       |  1  |   -1   |   -10   |   -8   |  30  |  12  |  80  |  -80  |   36   |   100   |   64  | 197 | 199 | 200 |
|      2       |  1  |   -1   |   -10   |   2    |  30  |  8   |  10  |   60  |   36   |   100   |   64  | 204 | 204 | 196 |
|      3       |  1  |   -1   |    5    |   -8   |  10  |  12  |  10  |   60  |   36   |   100   |   64  | 196 | 196 | 201 |
|      4       |  1  |   -1   |    5    |   2    |  10  |  8   |  80  |  -80  |   36   |   100   |   64  | 204 | 198 | 202 |
|      5       |  1  |   6    |   -10   |   -8   |  10  |  8   |  80  |   60  |   36   |   100   |   64  | 197 | 199 | 199 |
|      6       |  1  |   6    |   -10   |   2    |  10  |  12  |  10  |  -80  |   36   |   100   |   64  | 193 | 196 | 201 |
|      7       |  1  |   6    |    5    |   -8   |  30  |  8   |  10  |  -80  |   36   |   100   |   64  | 193 | 200 | 199 |
|      8       |  1  |   6    |    5    |   2    |  30  |  12  |  80  |   60  |   36   |   100   |   64  | 195 | 200 | 196 |
|      9       | 1.0 | 6.752  |   -2.5  |  -3.0  | 20.0 | 10.0 | 45.0 | -10.0 | 39.762 |   62.5  |  34.0 | 198 | 202 | 198 |
|      10      | 1.0 | -1.753 |   -2.5  |  -3.0  | 20.0 | 10.0 | 45.0 | -10.0 | 39.762 |   62.5  |  34.0 | 201 | 199 | 204 |
|      11      | 1.0 |  2.5   |  6.613  |  -3.0  | 20.0 | 10.0 | 45.0 | -10.0 |  18.5  | 108.062 |  34.0 | 202 | 201 | 196 |
|      12      | 1.0 |  2.5   | -11.612 |  -3.0  | 20.0 | 10.0 | 45.0 | -10.0 |  18.5  | 108.062 |  34.0 | 194 | 199 | 200 |
|      13      | 1.0 |  2.5   |   -2.5  | 3.075  | 20.0 | 10.0 | 45.0 | -10.0 |  18.5  |   62.5  | 70.45 | 193 | 201 | 195 |
|      14      | 1.0 |  2.5   |   -2.5  | -9.075 | 20.0 | 10.0 | 45.0 | -10.0 |  18.5  |   62.5  | 70.45 | 200 | 201 | 199 |
|      15      | 1.0 |  2.5   |   -2.5  |  -3.0  | 20.0 | 10.0 | 45.0 | -10.0 |  18.5  |   62.5  |  34.0 | 195 | 202 | 194 |
+--------------+-----+--------+---------+--------+------+------+------+-------+--------+---------+-------+-----+-----+-----+
#+end_src

** Statistical estimates of the results
For critical test values we use =scipy= module:
#+BEGIN_SRC python :session orthogonal
import scipy
from scipy.stats import f
from scipy.stats import t

def f_critical(prob, f1, f2):
    return scipy.stats.f.ppf(prob, f1, f2)

def t_critical(prob, df):
    return scipy.stats.t.ppf(prob, df)

def c_critical(prob, f1, f2):
    return 1 / (1 + (f2 - 1) / scipy.stats.f.ppf(1 - (1-prob)/f2, f1, (f2 - 1)*f1) )
#+END_SRC

*** Cochran's C test
To test variance homogeneity of response function by rows, we use Cochran's C test. Estimate of Cochran's test
is calculated as a ratio of maximal variance to the variances sum:
#+BEGIN_SRC python :session orthogonal
s2_y = [np.var(y[i]) for i in range(experiments)]
stat_c = max(s2_y) / sum(s2_y)
#+END_SRC

Let's calculate the critical value and compare it to the estimate:
#+BEGIN_SRC python :results output org :session orthogonal :exports both
crit_c = c_critical(confidence_prob, samples-1, experiments)

print(f"Calculated C statistics: {round(stat_c, 3)}")
print(
    f"Critical C for confidence probability of {confidence_prob}: {round(crit_c, 3)}"
)
#+END_SRC

#+RESULTS:
#+begin_src org
Calculated C statistics: 0.142
Critical C for confidence probability of 0.9: 0.301
#+end_src

The estimate is lower than critical value, hence variances are homogeneous. But what to do, if they
are not homogeneous? To prevent this, we envelop response generation code with =while= loop,
and on every negative result increase sample rate by one:
#+BEGIN_SRC python :results output org :session orthogonal :exports both
while True:
    y = [
        [random.randint(min(y_bounds), max(y_bounds)) for i in range(samples)]
        for j in range(experiments)
    ]
    matrix = create_matrix()
    s2_y = [np.var(y[i]) for i in range(experiments)]
    stat_c = max(s2_y) / sum(s2_y)
    crit_c = c_critical(confidence_prob, samples - 1, experiments)
    print(matrix)
    print(f"Calculated C statistics: {round(stat_c, 3)}")
    print(
        f"Critical C for confidence probability of {confidence_prob}: {round(crit_c, 3)}"
    )
    if stat_c < crit_c:
        print("Variances are equal.")
        break
    print("Variances are not equal. Increasing sample size...")
    samples += 1
#+END_SRC

#+RESULTS:
#+begin_src org
+--------------+-----+--------+---------+--------+------+------+------+-------+--------+---------+-------+-----+-----+-----+
| Experiment # |  x0 |   x1   |    x2   |   x3   |  x4  |  x5  |  x6  |   x7  |   x8   |    x9   |  x10  |  y1 |  y2 |  y3 |
+--------------+-----+--------+---------+--------+------+------+------+-------+--------+---------+-------+-----+-----+-----+
|      1       |  1  |   -1   |   -10   |   -8   |  30  |  12  |  80  |  -80  |   36   |   100   |   64  | 195 | 200 | 198 |
|      2       |  1  |   -1   |   -10   |   2    |  30  |  8   |  10  |   60  |   36   |   100   |   64  | 193 | 196 | 195 |
|      3       |  1  |   -1   |    5    |   -8   |  10  |  12  |  10  |   60  |   36   |   100   |   64  | 193 | 204 | 195 |
|      4       |  1  |   -1   |    5    |   2    |  10  |  8   |  80  |  -80  |   36   |   100   |   64  | 195 | 201 | 193 |
|      5       |  1  |   6    |   -10   |   -8   |  10  |  8   |  80  |   60  |   36   |   100   |   64  | 198 | 193 | 204 |
|      6       |  1  |   6    |   -10   |   2    |  10  |  12  |  10  |  -80  |   36   |   100   |   64  | 195 | 204 | 194 |
|      7       |  1  |   6    |    5    |   -8   |  30  |  8   |  10  |  -80  |   36   |   100   |   64  | 194 | 198 | 203 |
|      8       |  1  |   6    |    5    |   2    |  30  |  12  |  80  |   60  |   36   |   100   |   64  | 200 | 196 | 204 |
|      9       | 1.0 | 6.752  |   -2.5  |  -3.0  | 20.0 | 10.0 | 45.0 | -10.0 | 39.762 |   62.5  |  34.0 | 197 | 194 | 197 |
|      10      | 1.0 | -1.753 |   -2.5  |  -3.0  | 20.0 | 10.0 | 45.0 | -10.0 | 39.762 |   62.5  |  34.0 | 201 | 200 | 193 |
|      11      | 1.0 |  2.5   |  6.613  |  -3.0  | 20.0 | 10.0 | 45.0 | -10.0 |  18.5  | 108.062 |  34.0 | 204 | 202 | 202 |
|      12      | 1.0 |  2.5   | -11.612 |  -3.0  | 20.0 | 10.0 | 45.0 | -10.0 |  18.5  | 108.062 |  34.0 | 195 | 202 | 202 |
|      13      | 1.0 |  2.5   |   -2.5  | 3.075  | 20.0 | 10.0 | 45.0 | -10.0 |  18.5  |   62.5  | 70.45 | 201 | 199 | 197 |
|      14      | 1.0 |  2.5   |   -2.5  | -9.075 | 20.0 | 10.0 | 45.0 | -10.0 |  18.5  |   62.5  | 70.45 | 200 | 198 | 200 |
|      15      | 1.0 |  2.5   |   -2.5  |  -3.0  | 20.0 | 10.0 | 45.0 | -10.0 |  18.5  |   62.5  |  34.0 | 194 | 200 | 203 |
+--------------+-----+--------+---------+--------+------+------+------+-------+--------+---------+-------+-----+-----+-----+
Calculated C statistics: 0.154
Critical C for confidence probability of 0.9: 0.301
Variances are equal.
#+end_src

*** Regression equation and Student's t test
To calculate regression, we use mean combinations of each column. In the system of equations, 
equation matrix correspond to mean combinations of factors, while mean combinations of each
factor column with function means correspond to constant terms:
#+BEGIN_SRC python :session orthogonal
my = [np.mean(y[i]) for i in range(len(y))]

mxy = list(map(np.mean, x_col * my))
mxx = [
    [np.mean(x_col[i] * x_col[j]) for j in range(len(x_col))]
    for i in range(len(x_col))
]
#+END_SRC

We use columns in our previous calculations, but we need /row/ matrix
to use =numpy.linalg.solve()=, so we rotate =mxx= and find regression
coefficients:
#+BEGIN_SRC python :session orthogonal
equation_matrix = np.array(list(zip(*mxx)))
constant_terms = mxy

b = np.linalg.solve(equation_matrix, constant_terms)
#+END_SRC

To test, whether regression coefficients are right, we calculate test
values of function, which must be approximately equal to mean values:
#+BEGIN_SRC python :results output org :session orthogonal :exports both
y_test = [sum((b * np.array(x))[i]) for i in range(experiments)]

print(f"Means: {[round(my[i], 3) for i in range(experiments)]}")
print(f"Calculated function: {[round(y_test[i], 3) for i in range(experiments)]}")
#+END_SRC

#+RESULTS:
#+begin_src org
Means: [197.667, 194.667, 197.333, 196.333, 198.333, 197.667, 198.333, 200.0, 196.0, 198.0, 202.667, 199.667, 199.0, 199.333, 199.0]
Calculated function: [197.962, 195.09, 198.047, 197.175, 197.623, 197.085, 198.041, 199.836, 197.439, 196.129, 201.762, 200.139, 198.573, 199.328, 199.772]
#+end_src

Now we conduct Student's t test to find significant regression coefficients.
Let's find general recreation estimate, coefficients variance estimate and
Student's estimate:
#+BEGIN_SRC python :session orthogonal
beta = [sum(my * xn_col[i]) / experiments for i in range(factors)]
s2_b = sum(s2_y) / len(s2_y)
s_beta = np.sqrt(s2_b / samples / experiments)
stat_t = [abs(beta[i]) / s_beta for i in range(factors)]
#+END_SRC

Let's calculate the critical value and compare it to the estimate:
#+BEGIN_SRC python :results output org :session orthogonal :exports both
crit_t = t_critical(confidence_prob, (samples-1)*experiments)

print(f"Calculated t statistics: {[round(stat_t[i], 3) for i in range(len(stat_t))]}")
print(f"Critical t for confidence probability of {confidence_prob}: {round(crit_t, 3)}")
#+END_SRC

#+RESULTS:
#+begin_src org
Calculated t statistics: [422.154, 0.838, 1.038, 0.483, 0.142, 0.71, 0.615, 0.047, 306.887, 308.633, 307.795]
Critical t for confidence probability of 0.9: 1.31
#+end_src

As we can see, not all coefficients pass the test (=stat_t[i] > crit_t=).
Decoded coefficients, that don't pass the test we equate to zero, and
number of significant coefficients we write to a variable:
#+BEGIN_SRC python :results output org :session orthogonal :exports both
significant_coeffs = len(b)
for i in range(len(stat_t)):
    if stat_t[i] < crit_t:
        b[i] = 0
        significant_coeffs -= 1

print(f"Significant coefficients: {significant_coeffs}")
print(f"Regression coefficients: {[round(b[i], 3) for i in range(len(b))]}")
#+END_SRC

#+RESULTS:
#+begin_src org
Significant coefficients: 4
Regression coefficients: [197.105, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -0.141, 0.026, -0.023]
#+end_src

*** Fisher's F test
First, we calculate function values for found regression equation:
#+BEGIN_SRC python :results output org :session orthogonal :exports both
y_calc = [sum((b * np.array(x))[i]) for i in range(experiments)]

print(
    f"Calculated values of model: {[round(y_calc[i], 3) for i in range(len(y_calc))]}"
)
#+END_SRC

#+RESULTS:
#+begin_src org
Calculated values of model: [193.193, 193.193, 193.193, 193.193, 193.193, 193.193, 193.193, 193.193, 192.369, 192.369, 196.536, 196.536, 194.536, 194.536, 195.357]
#+end_src

Let's calculate adequate model variance and find Fisher's estimate, which equals to ratio of
adequate model variance to recreation variance:
#+BEGIN_SRC python :session orthogonal
s2_adeq = (
    samples
    / (experiments - significant_coeffs)
    * sum([(y_calc[i] - my[i]) ** 2 for i in range(experiments)])
)
stat_f = s2_adeq / s2_b
#+END_SRC

Let's calculate the critical value and compare it to the estimate:
#+BEGIN_SRC python :results output org :session orthogonal :exports both
crit_f = f_critical(confidence_prob, (samples-1)*experiments, experiments - significant_coeffs)

print(f"Calculated F statistics: {round(stat_f, 3)}")
print(f"Critical F for confidence probability of {confidence_prob}: {round(crit_f, 3)}")
#+END_SRC

#+RESULTS:
#+begin_src org
Calculated F statistics: 8.707
Critical F for confidence probability of 0.9: 2.076
#+end_src

As we can see, this model is not adequate. For that reason we create =bool= variable
=found= and envelop the whole procedure into =while= loop:
#+BEGIN_SRC python :results output org :session orthogonal :exports both
found = False

while not found:
    # Cochran's C test
    while True:
        y = [
            [random.randint(min(y_bounds), max(y_bounds)) for i in range(samples)]
            for j in range(experiments)
        ]

...
...
...

    print(f"Calculated F statistics: {round(stat_f, 3)}")
    print(f"Critical F for confidence probability of {confidence_prob}: {round(crit_f, 3)}")

    if stat_f > crit_f:
        print("Model is inadequate.")
    else:
        print("Model is adequate.")
        found = True
#+END_SRC

This code will generate adequate model.
